phoenix-srun: job 486955 queued and waiting for resources
phoenix-srun: job 486955 has been allocated resources
phoenix-srun: Job 486955 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal

[_init_petrel]-   1  cur: 0.000s, avg(1): 0.000s
[rank 0000]-[INFO]-[6401]-[2022-10-31 04:46:06]-[/spring/src/linklink/src/core.cc:220]: linklink init: world_size=1, rank=(0,0), device_num=1, thread_pool=1, buffer_pool=-1
[rank 0] >> task_info.group[0] ranks [0]
[rank 0] >> task_info.root_group ranks [0]
[rank 0] >> task_info.backbone_share_group[[0]] ranks [0]
[rank 0] >> task_info.neck_share_group[[0]] ranks [0]
[rank 0] >> task_info.decoder_share_group[[0]] ranks [0]
[rank 0] backbone of task0 has been overided to {'type': 'vit_base_patch16_ladder_attention_share_pos_embed', 'kwargs': {'task_sp_list': ['cls_token', 'cls_token_pos_embed', 'rel_pos_h', 'rel_pos_w'], 'pretrained': True, 'img_size': [224, 224], 'lms_checkpoint_train': 'fairscale', 'window': False, 'test_pos_mode': 'learnable_simple_interpolate', 'pad_attn_mask': False, 'round_padding': True, 'learnable_pos': True, 'drop_path_rate': 0.0, 'use_cls_token': True}}
[rank 0] neck of task0 has been overided to {'type': 'LadderSideAttentionFPN', 'kwargs': {'lms_checkpoint_train': 'fairscale', 'layer_feat_nums': 12, 'hidden_dim': 768, 'reduct_ration': 8, 'transformer_block_nums': 1, 'transformer_block_num_heads': 2, 'gate_T': 1.0, 'gate_alpha': 0, 'use_cls_token': True}}
[rank 0] decoder of task0 has been overided to {'type': 'reid_cls_vit_B', 'kwargs': {'use_sync_bn': True, 'bn_sync_stats': False, 'bn_momentum': 0.1, 'bn_eps': 1e-05, 'feature_bn': True, 'feature_only': False, 'out_feature': 768, 'loss_cfg': {'type': 'Softmax_TripletLoss', 'kwargs': {'in_features': 768, 'out_features': 3261, 'tri_margin': None, 'balance_weight': 1}}}}
[rank 0] dataset of task0 has been overided to {'type': 'ReIDDataset', 'kwargs': {'data_use_ratio': 1, 'task_spec': {'list': ['sh1424:s3://pedreid_public/market1501/data_list/fileList.txt', 'sh1424:s3://pedreid_public/dukemtmc-reid/data_list/fileList.txt', 'sh1424:s3://pedreid_public/cuhk03_1/data_list/fileList.txt', 'sh1424:s3://pedreid_public/MSMT17_V1/data_list/fileList.txt'], 'meta': ['sh1424:s3://pedreid_public/market1501/data_list/metaList.txt', 'sh1424:s3://pedreid_public/dukemtmc-reid/data_list/metaList.txt', 'sh1424:s3://pedreid_public/cuhk03_1/data_list/metaList.txt', 'sh1424:s3://pedreid_public/MSMT17_V1/data_list/metaList.txt'], 'prefix': ['sh1424:s3://pedreid_public/', 'sh1424:s3://pedreid_public/', 'sh1424:s3://pedreid_public/', 'sh1424:s3://pedreid_public/']}, 'augmentation': {'height': 256, 'width': 128, 'earser': True, 'brightness': False, 'contrast': False, 'vit': True, 'split': {'bg_type': 0, 'aug_type': 3, 'prob': 0.2}}, 'loader': 'pil'}}
[rank 0] sampler of task0 has been overided to {'type': 'RandomIdentity', 'batch_size': 112, 'shuffle_strategy': 6}
[rank 0] >> task_info.group[0] ranks [0]
[rank 0] >> task_info.root_group ranks [0]
[rank 0] >> task_info.backbone_share_group[[0]] ranks [0]
[rank 0] >> task_info.neck_share_group[[0]] ranks [0]
[rank 0] >> task_info.decoder_share_group[[0]] ranks [0]
[rank 0] dataset of task0 has been overided to {'type': 'ReIDTestDataset', 'kwargs': {'data_use_ratio': 1, 'root_path': 'sh1424:s3://pedreid_public/', 'query_file_path': ['sh1424:s3://pedreid_public/market1501/data_list/probe.txt'], 'gallery_file_path': ['sh1424:s3://pedreid_public/market1501/data_list/gallery.txt'], 'loader': 'pil', 'vit': True}}
sync_print: rank 0, override tensor.cuda() to preserve task_specific flag
[2022-10-31 16:46:09,376][     tester_deter.py][line:  47][    INFO] deterministic mode, seed: 2022, worker_rank: True,                                   cudnn_deterministic: False
building dataset from sh1424:s3://pedreid_public/market1501/data_list/probe.txt
Successfully Loaded Data, Totally 750 IDs
building dataset from sh1424:s3://pedreid_public/market1501/data_list/gallery.txt
Successfully Loaded Data, Totally 751 IDs
pos embed shape:  torch.Size([1, 196, 768])
Missing keys: ['cls_token_pos_embed']

finish load
sync_print: rank 0, Number of conv/bn params: 0.59M
sync_print: rank 0, Number of linear params: 85.02M
[rank 0] add param cls_token as task_specific
[rank 0] add param cls_token_pos_embed as task_specific
[rank 0] add param pos_embed as backbone_specific
[rank 0] add param patch_embed.proj.weight as backbone_specific
[rank 0] add param patch_embed.proj.bias as backbone_specific
[rank 0] add param blocks.0.norm1.weight as backbone_specific
[rank 0] add param blocks.0.norm1.bias as backbone_specific
[rank 0] add param blocks.0.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.0.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.0.attn.proj.weight as backbone_specific
[rank 0] add param blocks.0.attn.proj.bias as backbone_specific
[rank 0] add param blocks.0.norm2.weight as backbone_specific
[rank 0] add param blocks.0.norm2.bias as backbone_specific
[rank 0] add param blocks.0.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.0.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.0.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.0.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.1.norm1.weight as backbone_specific
[rank 0] add param blocks.1.norm1.bias as backbone_specific
[rank 0] add param blocks.1.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.1.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.1.attn.proj.weight as backbone_specific
[rank 0] add param blocks.1.attn.proj.bias as backbone_specific
[rank 0] add param blocks.1.norm2.weight as backbone_specific
[rank 0] add param blocks.1.norm2.bias as backbone_specific
[rank 0] add param blocks.1.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.1.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.1.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.1.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.2.norm1.weight as backbone_specific
[rank 0] add param blocks.2.norm1.bias as backbone_specific
[rank 0] add param blocks.2.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.2.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.2.attn.proj.weight as backbone_specific
[rank 0] add param blocks.2.attn.proj.bias as backbone_specific
[rank 0] add param blocks.2.norm2.weight as backbone_specific
[rank 0] add param blocks.2.norm2.bias as backbone_specific
[rank 0] add param blocks.2.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.2.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.2.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.2.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.3.norm1.weight as backbone_specific
[rank 0] add param blocks.3.norm1.bias as backbone_specific
[rank 0] add param blocks.3.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.3.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.3.attn.proj.weight as backbone_specific
[rank 0] add param blocks.3.attn.proj.bias as backbone_specific
[rank 0] add param blocks.3.norm2.weight as backbone_specific
[rank 0] add param blocks.3.norm2.bias as backbone_specific
[rank 0] add param blocks.3.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.3.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.3.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.3.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.4.norm1.weight as backbone_specific
[rank 0] add param blocks.4.norm1.bias as backbone_specific
[rank 0] add param blocks.4.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.4.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.4.attn.proj.weight as backbone_specific
[rank 0] add param blocks.4.attn.proj.bias as backbone_specific
[rank 0] add param blocks.4.norm2.weight as backbone_specific
[rank 0] add param blocks.4.norm2.bias as backbone_specific
[rank 0] add param blocks.4.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.4.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.4.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.4.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.5.norm1.weight as backbone_specific
[rank 0] add param blocks.5.norm1.bias as backbone_specific
[rank 0] add param blocks.5.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.5.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.5.attn.proj.weight as backbone_specific
[rank 0] add param blocks.5.attn.proj.bias as backbone_specific
[rank 0] add param blocks.5.norm2.weight as backbone_specific
[rank 0] add param blocks.5.norm2.bias as backbone_specific
[rank 0] add param blocks.5.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.5.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.5.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.5.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.6.norm1.weight as backbone_specific
[rank 0] add param blocks.6.norm1.bias as backbone_specific
[rank 0] add param blocks.6.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.6.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.6.attn.proj.weight as backbone_specific
[rank 0] add param blocks.6.attn.proj.bias as backbone_specific
[rank 0] add param blocks.6.norm2.weight as backbone_specific
[rank 0] add param blocks.6.norm2.bias as backbone_specific
[rank 0] add param blocks.6.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.6.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.6.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.6.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.7.norm1.weight as backbone_specific
[rank 0] add param blocks.7.norm1.bias as backbone_specific
[rank 0] add param blocks.7.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.7.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.7.attn.proj.weight as backbone_specific
[rank 0] add param blocks.7.attn.proj.bias as backbone_specific
[rank 0] add param blocks.7.norm2.weight as backbone_specific
[rank 0] add param blocks.7.norm2.bias as backbone_specific
[rank 0] add param blocks.7.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.7.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.7.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.7.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.8.norm1.weight as backbone_specific
[rank 0] add param blocks.8.norm1.bias as backbone_specific
[rank 0] add param blocks.8.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.8.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.8.attn.proj.weight as backbone_specific
[rank 0] add param blocks.8.attn.proj.bias as backbone_specific
[rank 0] add param blocks.8.norm2.weight as backbone_specific
[rank 0] add param blocks.8.norm2.bias as backbone_specific
[rank 0] add param blocks.8.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.8.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.8.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.8.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.9.norm1.weight as backbone_specific
[rank 0] add param blocks.9.norm1.bias as backbone_specific
[rank 0] add param blocks.9.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.9.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.9.attn.proj.weight as backbone_specific
[rank 0] add param blocks.9.attn.proj.bias as backbone_specific
[rank 0] add param blocks.9.norm2.weight as backbone_specific
[rank 0] add param blocks.9.norm2.bias as backbone_specific
[rank 0] add param blocks.9.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.9.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.9.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.9.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.10.norm1.weight as backbone_specific
[rank 0] add param blocks.10.norm1.bias as backbone_specific
[rank 0] add param blocks.10.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.10.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.10.attn.proj.weight as backbone_specific
[rank 0] add param blocks.10.attn.proj.bias as backbone_specific
[rank 0] add param blocks.10.norm2.weight as backbone_specific
[rank 0] add param blocks.10.norm2.bias as backbone_specific
[rank 0] add param blocks.10.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.10.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.10.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.10.mlp.fc2.bias as backbone_specific
[rank 0] add param blocks.11.norm1.weight as backbone_specific
[rank 0] add param blocks.11.norm1.bias as backbone_specific
[rank 0] add param blocks.11.attn.qkv.weight as backbone_specific
[rank 0] add param blocks.11.attn.qkv.bias as backbone_specific
[rank 0] add param blocks.11.attn.proj.weight as backbone_specific
[rank 0] add param blocks.11.attn.proj.bias as backbone_specific
[rank 0] add param blocks.11.norm2.weight as backbone_specific
[rank 0] add param blocks.11.norm2.bias as backbone_specific
[rank 0] add param blocks.11.mlp.fc1.weight as backbone_specific
[rank 0] add param blocks.11.mlp.fc1.bias as backbone_specific
[rank 0] add param blocks.11.mlp.fc2.weight as backbone_specific
[rank 0] add param blocks.11.mlp.fc2.bias as backbone_specific
[rank 0] add param norm.weight as backbone_specific
[rank 0] add param norm.bias as backbone_specific
[rank 0] add param reduction_layers.0.weight as neck_specific
[rank 0] add param reduction_layers.0.bias as neck_specific
[rank 0] add param reduction_layers.1.weight as neck_specific
[rank 0] add param reduction_layers.1.bias as neck_specific
[rank 0] add param reduction_layers.2.weight as neck_specific
[rank 0] add param reduction_layers.2.bias as neck_specific
[rank 0] add param reduction_layers.3.weight as neck_specific
[rank 0] add param reduction_layers.3.bias as neck_specific
[rank 0] add param reduction_layers.4.weight as neck_specific
[rank 0] add param reduction_layers.4.bias as neck_specific
[rank 0] add param reduction_layers.5.weight as neck_specific
[rank 0] add param reduction_layers.5.bias as neck_specific
[rank 0] add param reduction_layers.6.weight as neck_specific
[rank 0] add param reduction_layers.6.bias as neck_specific
[rank 0] add param reduction_layers.7.weight as neck_specific
[rank 0] add param reduction_layers.7.bias as neck_specific
[rank 0] add param reduction_layers.8.weight as neck_specific
[rank 0] add param reduction_layers.8.bias as neck_specific
[rank 0] add param reduction_layers.9.weight as neck_specific
[rank 0] add param reduction_layers.9.bias as neck_specific
[rank 0] add param reduction_layers.10.weight as neck_specific
[rank 0] add param reduction_layers.10.bias as neck_specific
[rank 0] add param reduction_layers.11.weight as neck_specific
[rank 0] add param reduction_layers.11.bias as neck_specific
[rank 0] add param reduction_layers.12.weight as neck_specific
[rank 0] add param reduction_layers.12.bias as neck_specific
[rank 0] add param side_gate_params.0 as neck_specific
[rank 0] add param side_gate_params.1 as neck_specific
[rank 0] add param side_gate_params.2 as neck_specific
[rank 0] add param side_gate_params.3 as neck_specific
[rank 0] add param side_gate_params.4 as neck_specific
[rank 0] add param side_gate_params.5 as neck_specific
[rank 0] add param side_gate_params.6 as neck_specific
[rank 0] add param side_gate_params.7 as neck_specific
[rank 0] add param side_gate_params.8 as neck_specific
[rank 0] add param side_gate_params.9 as neck_specific
[rank 0] add param side_gate_params.10 as neck_specific
[rank 0] add param side_gate_params.11 as neck_specific
[rank 0] add param transformer_blocks.0.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.0.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.0.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.0.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.0.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.0.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.1.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.1.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.2.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.2.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.3.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.3.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.4.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.4.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.5.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.5.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.6.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.6.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.7.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.7.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.8.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.8.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.9.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.9.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.10.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.10.0.mlp.fc2.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.norm1.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.norm1.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.attn.qkv.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.attn.proj.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.attn.proj.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.norm2.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.norm2.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.mlp.fc1.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.mlp.fc1.bias as neck_specific
[rank 0] add param transformer_blocks.11.0.mlp.fc2.weight as neck_specific
[rank 0] add param transformer_blocks.11.0.mlp.fc2.bias as neck_specific
[rank 0] add param last_proj.weight as neck_specific
[rank 0] add param last_proj.bias as neck_specific
[rank 0] add param feat_bn.weight as decoder_specific
[rank 0] add param feat_bn.bias as decoder_specific
[rank 0] add param loss.SoftmaxLoss.classifier.weight as decoder_specific
[rank 0] add buffer feat_bn.running_mean as decoder_specific
[rank 0] add buffer feat_bn.running_var as decoder_specific
backbone_aio_entry(
  (backbone_module): ViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln_pre): Identity()
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (neck_module): LadderSideAttentionFPN(
    (reduction_layers): ModuleList(
      (0): Linear(in_features=768, out_features=96, bias=True)
      (1): Linear(in_features=768, out_features=96, bias=True)
      (2): Linear(in_features=768, out_features=96, bias=True)
      (3): Linear(in_features=768, out_features=96, bias=True)
      (4): Linear(in_features=768, out_features=96, bias=True)
      (5): Linear(in_features=768, out_features=96, bias=True)
      (6): Linear(in_features=768, out_features=96, bias=True)
      (7): Linear(in_features=768, out_features=96, bias=True)
      (8): Linear(in_features=768, out_features=96, bias=True)
      (9): Linear(in_features=768, out_features=96, bias=True)
      (10): Linear(in_features=768, out_features=96, bias=True)
      (11): Linear(in_features=768, out_features=96, bias=True)
      (12): Linear(in_features=768, out_features=96, bias=True)
    )
    (side_gate_params): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
        (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
    )
    (transformer_blocks): ModuleList(
      (0): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (1): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (3): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (4): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (5): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (6): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (7): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (8): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (9): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (10): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (11): ModuleList(
        (0): TransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=96, out_features=288, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (attention_layers): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Identity()
      (4): Identity()
      (5): Identity()
      (6): Identity()
      (7): Identity()
      (8): Identity()
      (9): Identity()
      (10): Identity()
      (11): Identity()
      (12): Identity()
    )
    (last_proj): Linear(in_features=96, out_features=768, bias=True)
  )
  (decoder_module): reid_cls_vit_B(
    (feat_bn): SyncBatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, var_mode=syncbnVarMode_t.L2)
    (loss): Softmax(in_features=768, out_features=3261)TripletLoss(, margin=None)Softmax_TripletLoss(, balance_weight=1)
  )
)
[rank 0] broadcasting task-specific param module.backbone_module.cls_token	group_idx=1
[rank 0] broadcasting task-specific param module.backbone_module.cls_token_pos_embed	group_idx=1
[rank 0] broadcasting backbone-specific param module.backbone_module.pos_embed	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.patch_embed.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.patch_embed.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.0.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.1.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.2.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.3.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.4.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.5.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.6.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.7.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.8.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.9.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.10.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.norm1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.norm1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.attn.qkv.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.attn.qkv.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.attn.proj.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.attn.proj.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.norm2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.norm2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.mlp.fc1.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.mlp.fc1.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.mlp.fc2.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.blocks.11.mlp.fc2.bias	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.norm.weight	group_idx=3
[rank 0] broadcasting backbone-specific param module.backbone_module.norm.bias	group_idx=3
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.0.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.0.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.3.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.3.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.4.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.4.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.5.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.5.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.6.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.6.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.7.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.7.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.8.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.8.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.9.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.9.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.10.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.10.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.11.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.11.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.12.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.reduction_layers.12.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.0	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.1	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.2	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.3	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.4	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.5	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.6	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.7	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.8	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.9	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.10	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.side_gate_params.11	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.0.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.1.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.2.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.3.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.4.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.5.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.6.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.7.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.8.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.9.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.10.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.norm1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.norm1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.attn.qkv.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.attn.proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.attn.proj.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.norm2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.norm2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.mlp.fc1.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.mlp.fc1.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.mlp.fc2.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.transformer_blocks.11.0.mlp.fc2.bias	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.last_proj.weight	group_idx=4
[rank 0] broadcasting neck-specific param module.neck_module.last_proj.bias	group_idx=4
[rank 0] broadcasting decoder-specific param module.decoder_module.feat_bn.weight	group_idx=5
[rank 0] broadcasting decoder-specific param module.decoder_module.feat_bn.bias	group_idx=5
[rank 0] broadcasting decoder-specific param module.decoder_module.loss.SoftmaxLoss.classifier.weight	group_idx=5
[rank 0] broadcasting decoder-specific buffer module.decoder_module.feat_bn.running_mean
[rank 0] broadcasting decoder-specific buffer module.decoder_module.feat_bn.running_var
=> loading checkpoint '/mnt/lustre/chencheng1/expr_files/vitruvian/L2_full_setting_joint/checkpoints/v100_32g_vitbase_size224_lr1e3_stepLRx3_bmp1_adafactor_wd01_clip05_layerdecay075_lpe_peddet_citypersons_LSA_reduct8_tbn1_heads2_gate1_peddetShareDecoder_exp3_setting_SharePosEmbed/ckpt_task20_iter_newest.pth.tar'
Extract Features: [10/31]	task0 : reid_4sets	Time 3.166 (ETA:70.36h) (2.635)	
Extract Features: [20/31]	task0 : reid_4sets	Time 0.411 (ETA:9.12h) (0.113)	
Extract Features: [30/31]	task0 : reid_4sets	Time 0.399 (ETA:8.87h) (0.102)	
[_init_petrel]-   1  cur: 0.000s, avg(1): 0.000s
==> will load files from local machine
mc-support-ceph
[_init_petrel]-   1  cur: 0.000s, avg(1): 0.000s
==> will load files from local machine
mc-support-ceph
Extract Features: [10/143]	task0 : reid_4sets	Time 3.005 (ETA:66.76h) (2.707)	
Extract Features: [20/143]	task0 : reid_4sets	Time 0.483 (ETA:10.73h) (0.186)	
Extract Features: [30/143]	task0 : reid_4sets	Time 0.491 (ETA:10.91h) (0.195)	
Extract Features: [40/143]	task0 : reid_4sets	Time 0.395 (ETA:8.77h) (0.097)	
Extract Features: [50/143]	task0 : reid_4sets	Time 0.394 (ETA:8.74h) (0.096)	
Extract Features: [60/143]	task0 : reid_4sets	Time 0.392 (ETA:8.70h) (0.094)	
Extract Features: [70/143]	task0 : reid_4sets	Time 0.408 (ETA:9.06h) (0.111)	
Extract Features: [80/143]	task0 : reid_4sets	Time 0.429 (ETA:9.52h) (0.132)	
Extract Features: [90/143]	task0 : reid_4sets	Time 0.402 (ETA:8.93h) (0.103)	
Extract Features: [100/143]	task0 : reid_4sets	Time 0.399 (ETA:8.86h) (0.101)	
Extract Features: [110/143]	task0 : reid_4sets	Time 0.394 (ETA:8.74h) (0.096)	
Extract Features: [120/143]	task0 : reid_4sets	Time 0.403 (ETA:8.94h) (0.105)	
Extract Features: [130/143]	task0 : reid_4sets	Time 0.398 (ETA:8.82h) (0.099)	
Extract Features: [140/143]	task0 : reid_4sets	Time 0.438 (ETA:9.72h) (0.139)	
[_init_petrel]-   1  cur: 0.000s, avg(1): 0.000s
==> will load files from local machine
mc-support-ceph
[_init_petrel]-   1  cur: 0.000s, avg(1): 0.000s
==> will load files from local machine
mc-support-ceph
The test feature is normalized
=> Computing DistMat with euclidean_distance
Validation Results 
mAP: 88.4%
CMC curve, Rank-1  :94.3%
CMC curve, Rank-5  :98.5%
CMC curve, Rank-10 :99.2%
[rank 0000]-[INFO]-[6401]-[2022-10-31 04:48:26]-[/spring/src/linklink/src/core.cc:260]: linklink finalized!
